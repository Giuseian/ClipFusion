{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10415611,"sourceType":"datasetVersion","datasetId":6455388},{"sourceId":10415614,"sourceType":"datasetVersion","datasetId":6455391}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# all necessary imports\nimport numpy as np \nimport pandas as pd \nimport re\nfrom collections import Counter\nimport os\nimport json\nimport string\nimport spacy\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\nfrom transformers import DistilBertModel\nfrom torch.utils.data import Dataset\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoTokenizer, BertModel, AdamW, CLIPProcessor, CLIPModel\nfrom torch.utils.data import DataLoader\nfrom argparse import Namespace\nfrom pathlib import Path\nimport aiohttp\nfrom transformers import T5Model\nfrom torch.amp import autocast, GradScaler\nimport asyncio\nimport aiofiles\nfrom tqdm.asyncio import tqdm_asyncio\nimport requests\nimport subprocess\nfrom collections import Counter\nfrom tqdm import tqdm\nimport cv2\nfrom PIL import Image \nimport time\nfrom requests.exceptions import HTTPError\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"ee356b61-673f-4172-a74f-758950d2efbb","_cell_guid":"89665e0c-8f9b-44db-975e-c0c111a03858","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:46:51.067187Z","iopub.execute_input":"2025-01-09T20:46:51.067472Z","iopub.status.idle":"2025-01-09T20:47:09.930381Z","shell.execute_reply.started":"2025-01-09T20:46:51.067442Z","shell.execute_reply":"2025-01-09T20:47:09.929685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"_uuid":"b745683b-8e62-4316-af85-04233da5e78d","_cell_guid":"25935205-7782-4b06-8c76-d8af4d093db2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:09.930922Z","iopub.execute_input":"2025-01-09T20:47:09.931450Z","iopub.status.idle":"2025-01-09T20:47:09.934937Z","shell.execute_reply.started":"2025-01-09T20:47:09.931427Z","shell.execute_reply":"2025-01-09T20:47:09.933890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:47:09.935867Z","iopub.execute_input":"2025-01-09T20:47:09.936141Z","iopub.status.idle":"2025-01-09T20:47:09.955977Z","shell.execute_reply.started":"2025-01-09T20:47:09.936099Z","shell.execute_reply":"2025-01-09T20:47:09.955194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade ipywidgets","metadata":{"_uuid":"8b3eaf95-d566-4481-9d37-d0cfb8532d90","_cell_guid":"b49868dd-26df-4380-ae9c-df2a3f1f08f4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:09.956788Z","iopub.execute_input":"2025-01-09T20:47:09.957057Z","iopub.status.idle":"2025-01-09T20:47:14.408932Z","shell.execute_reply.started":"2025-01-09T20:47:09.957030Z","shell.execute_reply":"2025-01-09T20:47:14.408055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!jupyter nbextension install --py --symlink --sys-prefix ipywidgets\n!jupyter nbextension enable --py --sys-prefix ipywidgets","metadata":{"_uuid":"b7a70f7f-4c8a-42a9-8fe6-41dcb15af4e5","_cell_guid":"fe65832b-03c0-4a90-86e8-ce52268003fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:14.409818Z","iopub.execute_input":"2025-01-09T20:47:14.410033Z","iopub.status.idle":"2025-01-09T20:47:15.760345Z","shell.execute_reply.started":"2025-01-09T20:47:14.410015Z","shell.execute_reply":"2025-01-09T20:47:15.759251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"_uuid":"67c45b2c-86c2-4e26-bf31-b87c250c5417","_cell_guid":"94eee963-e6a2-4cbb-b690-377304b48a4f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:15.762692Z","iopub.execute_input":"2025-01-09T20:47:15.762920Z","iopub.status.idle":"2025-01-09T20:47:19.155016Z","shell.execute_reply.started":"2025-01-09T20:47:15.762901Z","shell.execute_reply":"2025-01-09T20:47:19.153922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"694c93a3-4bc3-443b-8c46-7977fb5bd968","_cell_guid":"58adba4a-00d5-4d72-86db-c6923c648e71","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:19.157217Z","iopub.execute_input":"2025-01-09T20:47:19.157498Z","iopub.status.idle":"2025-01-09T20:47:19.161443Z","shell.execute_reply.started":"2025-01-09T20:47:19.157476Z","shell.execute_reply":"2025-01-09T20:47:19.160552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"utils -> https://www.kaggle.com/code/majinx/nlp-eda-tfidf-wordcloud-lemmatization","metadata":{"_uuid":"3851dcd4-d117-4adb-a215-e50e8acf3022","_cell_guid":"e4dc1d6b-4cb3-481d-80ca-41d64a705ebd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 1. GET DATA","metadata":{"_uuid":"f6c27ff7-fc32-4137-830a-e371a4874ca3","_cell_guid":"de62a972-452c-43b3-a690-f86ca56dbd14","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"#### tgif-mc","metadata":{"_uuid":"1e180e9c-4dd5-4336-a2f4-48f9f14017ab","_cell_guid":"0f4909f2-89a7-4c6e-946c-dae8e4ccdbde","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"tgif-mc dataset is divided into 2 main multiple choice tasks:\n- REPEATING ACTION \n    - example: \"What does the cat do 3 times?\" - \"Put head down\"\n    - train: 20,475, test: 2,274, total: 22,749\n- STATE TRANSITION \n    - example: \"What does the model do after lower coat?\" - \"Pivot around\"\n    - train: 52,704, test: 6,232, total: 58,936\n\nIn this notebook the focus is on RA part","metadata":{"_uuid":"316f29fb-5b0b-4f4e-9a0e-f00e763e1088","_cell_guid":"60381234-5ddb-4b23-b619-868c0c11a0cb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def clean_dataframe(file_path, column_mapping, id_prefix='mc'):\n    \"\"\"\n    Preprocess the test DataFrame by renaming columns, reordering columns, and modifying IDs.\n\n    Args:\n        file_path (str): Path to the CSV file to be loaded.\n        column_mapping (dict): Dictionary for renaming columns (e.g., {'gif_name': 'title', 'vid_id': 'clip_name', 'key': 'id'}).\n        id_prefix (str): Prefix to add to IDs in the 'id' column.\n\n    Returns:\n        pd.DataFrame: Preprocessed DataFrame.\n    \"\"\"\n    # Load the CSV file\n    df = pd.read_csv(file_path, sep='\\t')\n\n    # Rename columns\n    df = df.rename(columns=column_mapping)\n\n    # Modify 'id' column\n    if 'id' in df.columns:\n        df['id'] = df['id'].apply(lambda x: f'{id_prefix}{x}' if pd.notnull(x) else x)\n\n    # Reorder columns \n    expected_columns = ['title', 'id', 'clip_name', 'question', 'a1', 'a2', 'a3', 'a4', 'a5', 'answer']\n    df = df[[col for col in expected_columns if col in df.columns]]\n\n    return df","metadata":{"_uuid":"f9ae59e6-8faa-4a16-b5c8-34f5337f9824","_cell_guid":"9d684fa4-4b35-468a-88e3-70e97d05fa76","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:19.162227Z","iopub.execute_input":"2025-01-09T20:47:19.162466Z","iopub.status.idle":"2025-01-09T20:47:19.177678Z","shell.execute_reply.started":"2025-01-09T20:47:19.162446Z","shell.execute_reply":"2025-01-09T20:47:19.176775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_mapping = {\n    'gif_name': 'title',\n    'vid_id': 'clip_name',\n    'key': 'id'\n}","metadata":{"_uuid":"281fb75b-67f9-4bee-b429-5bec37b11011","_cell_guid":"47347279-0847-4c65-ba61-839cd1f9558c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:19.178425Z","iopub.execute_input":"2025-01-09T20:47:19.178682Z","iopub.status.idle":"2025-01-09T20:47:19.194066Z","shell.execute_reply.started":"2025-01-09T20:47:19.178663Z","shell.execute_reply":"2025-01-09T20:47:19.193200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# REPEATING ACTION DATA\ntrain_ra = '/kaggle/input/tgif-mc/Train_action_question.csv'\ntest_ra = '/kaggle/input/tgif-mc/Test_action_question.csv'\n\n# Clean the DataFrames\ndf_train_ra = clean_dataframe(train_ra, column_mapping)\ndf_test_ra = clean_dataframe(test_ra, column_mapping)\n\n# Display the result\nprint(df_train_ra.head())\nprint(df_test_ra.head())","metadata":{"_uuid":"afe3bf4a-3961-491b-b7f8-acde9cb1a643","_cell_guid":"62bd92d3-d514-4e85-a316-e29eb26b7be7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:19.194937Z","iopub.execute_input":"2025-01-09T20:47:19.195258Z","iopub.status.idle":"2025-01-09T20:47:19.390818Z","shell.execute_reply.started":"2025-01-09T20:47:19.195227Z","shell.execute_reply":"2025-01-09T20:47:19.390104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_size = 10000\nsubset_df = df_train_ra[:subset_size]  # Select the first 10000 rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:47:19.391527Z","iopub.execute_input":"2025-01-09T20:47:19.391726Z","iopub.status.idle":"2025-01-09T20:47:19.395369Z","shell.execute_reply.started":"2025-01-09T20:47:19.391709Z","shell.execute_reply":"2025-01-09T20:47:19.394587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_test = 1000\nsubset_df_test = df_test_ra[:subset_test]  # Select the first 1000 rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:47:19.396188Z","iopub.execute_input":"2025-01-09T20:47:19.396403Z","iopub.status.idle":"2025-01-09T20:47:19.410613Z","shell.execute_reply.started":"2025-01-09T20:47:19.396385Z","shell.execute_reply":"2025-01-09T20:47:19.409784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. PREPROCESS DATA","metadata":{"_uuid":"8d4f41f3-ea82-43f8-8ad9-f56dff348c02","_cell_guid":"10e3820c-db71-45ca-9b9c-0d82d62befbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Let's define a single preprocessig fucntion, so to have more compactness","metadata":{"_uuid":"1aba1293-c6c7-4cab-88c1-8f50c5b91305","_cell_guid":"91c7934a-43b6-4010-b47f-45d6a3fc79d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load necessary resources\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nnlp = spacy.load('en_core_web_sm')","metadata":{"_uuid":"bb61c55c-d2a7-4af1-b569-5879e9127a08","_cell_guid":"5d418d8c-9494-4251-b733-1444bd5908a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:19.428097Z","iopub.execute_input":"2025-01-09T20:47:19.428359Z","iopub.status.idle":"2025-01-09T20:47:20.208723Z","shell.execute_reply.started":"2025-01-09T20:47:19.428341Z","shell.execute_reply":"2025-01-09T20:47:20.208049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_dataframe(df, text_columns, remove_stopwords=True, lemmatize=True, tokenize=True):\n    \"\"\"\n    Preprocess a DataFrame by cleaning text data in specified columns.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing text data.\n        text_columns (list): List of column names to preprocess.\n        remove_stopwords (bool): Whether to remove stopwords.\n        lemmatize (bool): Whether to lemmatize text.\n        tokenize (bool): Whether to tokenize text.\n\n    Returns:\n        pd.DataFrame: Preprocessed DataFrame.\n    \"\"\"\n\n    def contains_special_characters(value):\n        \"\"\"Check for special characters in a string.\"\"\"\n        pattern = r'[^a-zA-Z0-9?\\s]'\n        return bool(re.search(pattern, value)) if isinstance(value, str) else False\n\n    def remove_and_concatenate(text):\n        \"\"\"Remove 'tumblr' and concatenate remaining parts.\"\"\"\n        parts = text.split()\n        if len(parts) > 1 and parts[0].lower() == 'tumblr':\n            return ''.join(parts[1:])\n        return text\n\n    def remove_punctuation(text):\n        \"\"\"Remove punctuation from text.\"\"\"\n        return text.translate(str.maketrans('', '', string.punctuation))\n\n    def remove_stopwords_func(text):\n        \"\"\"Remove stopwords from text.\"\"\"\n        return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n\n    def lemmatize_with_spacy(text):\n        \"\"\"Lemmatize text using Spacy.\"\"\"\n        doc = nlp(text)\n        return ' '.join([token.lemma_ for token in doc])\n\n    def tokenize_text(text):\n        \"\"\"Tokenize text.\"\"\"\n        return word_tokenize(text) if isinstance(text, str) else []\n\n    # Remove NaN values\n    df = df.dropna(subset=text_columns)\n\n    # Replace special characters\n    for col in text_columns:\n        df[col] = df[col].apply(lambda x: remove_and_concatenate(str(x)) if contains_special_characters(x) else str(x))\n\n    # Convert to lowercase\n    df[text_columns] = df[text_columns].map(str.lower)\n\n    # Remove punctuation\n    df[text_columns] = df[text_columns].map(remove_punctuation)\n\n    # Remove extra whitespace\n    df[text_columns] = df[text_columns].map(str.strip)\n\n    # Remove stopwords\n    if remove_stopwords:\n        df[text_columns] = df[text_columns].map(remove_stopwords_func)\n\n    # Lemmatize\n    if lemmatize:\n        for col in text_columns:\n            df[col] = df[col].apply(lemmatize_with_spacy)\n\n    # Tokenize\n    if tokenize:\n        for col in text_columns:\n            df[col] = df[col].apply(tokenize_text)\n\n    return df","metadata":{"_uuid":"ebc33ce7-27f0-4327-8b67-98df9ac08043","_cell_guid":"6e5bf7f2-3be0-44ac-ac9a-26c09aa65f09","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:20.209515Z","iopub.execute_input":"2025-01-09T20:47:20.209822Z","iopub.status.idle":"2025-01-09T20:47:20.217887Z","shell.execute_reply.started":"2025-01-09T20:47:20.209792Z","shell.execute_reply":"2025-01-09T20:47:20.217074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Columns to preprocess\ntext_columns = ['title', 'question', 'a1', 'a2', 'a3', 'a4', 'a5']","metadata":{"_uuid":"083ab336-b574-4862-90a5-7dd4d3665955","_cell_guid":"e60fc72b-1be6-456c-bc71-f8688adcd146","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:20.218755Z","iopub.execute_input":"2025-01-09T20:47:20.218994Z","iopub.status.idle":"2025-01-09T20:47:20.235839Z","shell.execute_reply.started":"2025-01-09T20:47:20.218976Z","shell.execute_reply":"2025-01-09T20:47:20.234942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# REPEATING ACTION -> preprocessing\npreprocessed_df_train_ra = preprocess_dataframe(subset_df, text_columns)\npreprocessed_df_test_ra = preprocess_dataframe(df_test_ra, text_columns)","metadata":{"_uuid":"85e483a6-2b46-4369-85e3-e402cf6f7c94","_cell_guid":"ed472c3f-aba5-476a-a2d5-900e29930d4f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:47:20.236561Z","iopub.execute_input":"2025-01-09T20:47:20.236763Z","iopub.status.idle":"2025-01-09T20:50:14.114217Z","shell.execute_reply.started":"2025-01-09T20:47:20.236745Z","shell.execute_reply":"2025-01-09T20:50:14.113227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessed_df_train_ra.head()","metadata":{"_uuid":"cce1f187-7809-446b-9d3d-1e641fd69393","_cell_guid":"97284fee-bdea-4622-94b7-1f0c0691862c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:14.115151Z","iopub.execute_input":"2025-01-09T20:50:14.115401Z","iopub.status.idle":"2025-01-09T20:50:14.135103Z","shell.execute_reply.started":"2025-01-09T20:50:14.115380Z","shell.execute_reply":"2025-01-09T20:50:14.134211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(preprocessed_df_train_ra[\"answer\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:14.136005Z","iopub.execute_input":"2025-01-09T20:50:14.136388Z","iopub.status.idle":"2025-01-09T20:50:14.150246Z","shell.execute_reply.started":"2025-01-09T20:50:14.136354Z","shell.execute_reply":"2025-01-09T20:50:14.149411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. CREATE MC DATASET","metadata":{"_uuid":"957c2244-6100-4ccf-895c-aadad1dce181","_cell_guid":"8e95af4a-8105-43f8-91fc-459b6c344394","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MultipleChoiceDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length=128):\n        \"\"\"\n        Args:\n            dataset: A Pandas DataFrame or similar structure with the dataset.\n            tokenizer: A tokenizer instance (e.g., from HuggingFace Transformers).\n            max_length: Maximum sequence length for tokenization.\n        \"\"\"\n        self.dataset = dataset  # Use 'dataset' instead of 'data'\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, (int, np.integer)): \n            # Fetch a single row (adjust this based on your data structure)\n            row = self.dataset.iloc[idx]\n    \n            # Extract fields for question, choices, and correct answer\n            question = row[\"question\"]  \n            choices = [row['a1'], row['a2'], row['a3'], row['a4'], row['a5']]\n            correct_answer = row[\"answer\"]  \n            \n            # Tokenize each question-choice pair\n            inputs = self.tokenizer(\n                [f\"{question} {choice}\" for choice in choices],\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n    \n            # Return tokenized inputs and labels\n            return {\n                \"input_ids\": inputs[\"input_ids\"],  # Shape: (num_choices, seq_length)\n                \"attention_mask\": inputs[\"attention_mask\"],  # Shape: (num_choices, seq_length)\n                \"labels\": torch.tensor(correct_answer, dtype=torch.long)  \n            }\n\n        elif isinstance(idx, (list, np.ndarray)):  # Multiple indices\n            batch = [self.__getitem__(i) for i in idx]  # Recursively fetch rows\n            return {\n                \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n                \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n                \"labels\": torch.stack([item[\"labels\"] for item in batch]),\n            }","metadata":{"_uuid":"fe8736d8-873f-4655-9334-a269300caee6","_cell_guid":"42582490-e8e4-40e6-ad0c-7cbcf66fc7e3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:14.168882Z","iopub.execute_input":"2025-01-09T20:50:14.169100Z","iopub.status.idle":"2025-01-09T20:50:14.181501Z","shell.execute_reply.started":"2025-01-09T20:50:14.169081Z","shell.execute_reply":"2025-01-09T20:50:14.180698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset_ra = MultipleChoiceDataset(preprocessed_df_train_ra, tokenizer)\ntest_dataset_ra = MultipleChoiceDataset(preprocessed_df_test_ra, tokenizer)","metadata":{"_uuid":"86913370-d169-4301-b409-e3d6533078bc","_cell_guid":"895e7423-0b13-4440-a3b9-4701b3f849a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:14.182418Z","iopub.execute_input":"2025-01-09T20:50:14.182734Z","iopub.status.idle":"2025-01-09T20:50:21.341014Z","shell.execute_reply.started":"2025-01-09T20:50:14.182679Z","shell.execute_reply":"2025-01-09T20:50:21.340106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract labels from train_dataset_ra\nlabels = []\nfor i in range(len(train_dataset_ra)):\n    labels.append(train_dataset_ra[i][\"labels\"].item())  # Access label as an integer\n\n# Count occurrences of each label\nlabel_counts = Counter(labels)\n\n# Print the counts\nfor label, count in label_counts.items():\n    print(f\"Label {label}: {count} occurrences\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:21.341780Z","iopub.execute_input":"2025-01-09T20:50:21.341990Z","iopub.status.idle":"2025-01-09T20:50:25.785571Z","shell.execute_reply.started":"2025-01-09T20:50:21.341972Z","shell.execute_reply":"2025-01-09T20:50:25.784871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. TEXT ENCODER","metadata":{}},{"cell_type":"code","source":"class MaskedAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super(MaskedAttention, self).__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, attention_mask=None):\n        if attention_mask is not None:\n            # Shape: [batch_size, seq_len]\n            batch_size, seq_len = attention_mask.size()\n            #print('batch_size', batch_size) -> 40 (old_batch_size * num_choices)\n\n            # Expand mask for multi-head attention\n            # Shape: [batch_size, 1, 1, seq_len]\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n            # Broadcast to match [batch_size, num_heads, seq_len, seq_len]\n            attention_mask = attention_mask.expand(batch_size, self.num_heads, seq_len, seq_len)\n\n            # Convert to the correct format for MultiheadAttention\n            # Collapse `batch_size * num_heads` into one dimension\n            attention_mask = attention_mask.reshape(-1, seq_len, seq_len)\n\n            # Set masked values to -inf and others to 0\n            attention_mask = attention_mask.float().masked_fill(attention_mask == 0, float('-inf')).masked_fill(attention_mask == 1, 0)\n\n        # Perform multi-head attention\n        attn_output, _ = self.multihead_attn(x, x, x, attn_mask=attention_mask)\n        x = self.layer_norm(x + self.dropout(attn_output))  # Add & Norm\n\n        # Feed-forward network\n        ff_output = self.feed_forward(x)\n        return self.layer_norm(x + self.dropout(ff_output))  # Add & Norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:25.786428Z","iopub.execute_input":"2025-01-09T20:50:25.786705Z","iopub.status.idle":"2025-01-09T20:50:25.793619Z","shell.execute_reply.started":"2025-01-09T20:50:25.786683Z","shell.execute_reply":"2025-01-09T20:50:25.792865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, bert_model_name, num_heads, num_layers, max_length, embed_dim, dropout=0.1):\n        \"\"\"\n        Initialize the TextEncoder with BERT embeddings and masked attention layers.\n        \n        Args:\n            bert_model_name (str): Name of the pre-trained BERT model (e.g., 'bert-base-uncased').\n            num_heads (int): Number of attention heads.\n            num_layers (int): Number of masked attention layers.\n            max_length (int): Maximum sequence length.\n            embed_dim (int): Embedding dimension (should match BERT hidden size).\n            dropout (float): Dropout rate for attention layers.\n        \"\"\"\n        super(TextEncoder, self).__init__()\n        # Load pre-trained BERT model\n        self.bert = BertModel.from_pretrained(bert_model_name)\n\n        # Ensure BERT embeddings match the input dimension\n        assert embed_dim == self.bert.config.hidden_size, \\\n            f\"embed_dim ({embed_dim}) must match BERT hidden size ({self.bert.config.hidden_size}).\"\n\n        # Masked attention layers\n        self.layers = nn.ModuleList([\n            MaskedAttention(embed_dim, num_heads, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n\n        # Fully connected output layer\n        self.fc_out = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, input_ids, attention_mask, num_choices=5):\n        \"\"\"\n        Forward pass for the TextEncoder.\n\n        Args:\n            input_ids (torch.Tensor): Input token IDs of shape [batch_size, seq_len].\n            attention_mask (torch.Tensor): Attention mask of shape [batch_size, seq_len].\n\n        Returns:\n            torch.Tensor: Output embeddings of shape [batch_size, seq_len, embed_dim].\n        \"\"\"\n        # Get BERT embeddings\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        x = bert_output.last_hidden_state  # Shape: [batch_size, seq_len, embed_dim]\n\n        # Pass through masked attention layers\n        for layer in self.layers:\n            x = layer(x, attention_mask=attention_mask)\n\n        x = self.fc_out(x) # shape: [batch_size*num_choices, seq_len, embed_dim]\n\n        # Reshape back to [batch_size, num_choices, seq_len, embed_dim]\n        #batch_size = input_ids.size(0) // num_choices\n        #x = x.view(batch_size, num_choices, x.size(1), x.size(2))\n        \n        # Final output \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:25.794334Z","iopub.execute_input":"2025-01-09T20:50:25.794561Z","iopub.status.idle":"2025-01-09T20:50:25.814571Z","shell.execute_reply.started":"2025-01-09T20:50:25.794542Z","shell.execute_reply":"2025-01-09T20:50:25.813768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_encoder = TextEncoder(\n    bert_model_name=\"bert-base-uncased\",\n    num_heads=8,\n    num_layers=2,\n    max_length=128, \n    embed_dim=768\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:25.815492Z","iopub.execute_input":"2025-01-09T20:50:25.815694Z","iopub.status.idle":"2025-01-09T20:50:28.759885Z","shell.execute_reply.started":"2025-01-09T20:50:25.815676Z","shell.execute_reply":"2025-01-09T20:50:28.759192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextEncoderWithDistilBERT(nn.Module):\n    def __init__(self, bert_model_name, num_heads, num_layers, max_length, embed_dim, dropout=0.1):\n        \"\"\"\n        Initialize the TextEncoder with DistilBERT embeddings and masked attention layers.\n        \n        Args:\n            bert_model_name (str): Name of the pre-trained DistilBERT model.\n            num_heads (int): Number of attention heads.\n            num_layers (int): Number of masked attention layers.\n            max_length (int): Maximum sequence length.\n            embed_dim (int): Embedding dimension (should match DistilBERT hidden size).\n            dropout (float): Dropout rate for attention layers.\n        \"\"\"\n        super(TextEncoderWithDistilBERT, self).__init__()\n        # Load pre-trained DistilBERT model\n        self.distilbert = DistilBertModel.from_pretrained(bert_model_name)\n\n        # Ensure embeddings match the input dimension\n        assert embed_dim == self.distilbert.config.hidden_size, \\\n            f\"embed_dim ({embed_dim}) must match DistilBERT hidden size ({self.distilbert.config.hidden_size}).\"\n\n        # Masked attention layers\n        self.layers = nn.ModuleList([\n            MaskedAttention(embed_dim, num_heads, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n\n        # Fully connected output layer\n        self.fc_out = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, input_ids, attention_mask, num_choices=5):\n        \"\"\"\n        Forward pass for the TextEncoder with DistilBERT.\n\n        Args:\n            input_ids (torch.Tensor): Input token IDs of shape [batch_size, seq_len].\n            attention_mask (torch.Tensor): Attention mask of shape [batch_size, seq_len].\n\n        Returns:\n            torch.Tensor: Output embeddings of shape [batch_size, seq_len, embed_dim].\n        \"\"\"\n        # Get DistilBERT embeddings\n        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        x = distilbert_output.last_hidden_state  # Shape: [batch_size, seq_len, embed_dim]\n\n        # Pass through masked attention layers\n        for layer in self.layers:\n            x = layer(x, attention_mask=attention_mask)\n\n        x = self.fc_out(x)  # Shape: [batch_size*num_choices, seq_len, embed_dim]\n\n        # Final output \n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize TextEncoder with DistilBERT\ntext_encoder_db = TextEncoderWithDistilBERT(\n    bert_model_name=\"distilbert-base-uncased\",\n    num_heads=8,\n    num_layers=2,\n    max_length=128, \n    embed_dim=768\n).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(text_encoder.parameters(), lr=5e-5)\n\n# Training parameters\nepochs = 10  # Adjust \nbatch_size = 16 \ntrain_loader = DataLoader(train_dataset_ra, batch_size=batch_size, shuffle=True)\n\n# Training loop\nfor epoch in range(epochs):\n    text_encoder.train()  # Set the model to training mode\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    # Training Loop\n    num_choices = 5  # Number of choices per question\n    \n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(device)  # Shape: [batch_size, num_choices, seq_len]\n        attention_mask = batch[\"attention_mask\"].to(device)  # Same shape as input_ids\n        labels = batch[\"labels\"].to(device)  # Shape: [batch_size]\n    \n        # Flatten choices dimension for input to text_encoder\n        input_ids = input_ids.view(-1, input_ids.size(-1))  # [batch_size * num_choices, seq_len]\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1))  # Same shape\n    \n        # Forward pass through the text encoder\n        outputs = text_encoder(input_ids, attention_mask)  # [batch_size * num_choices, seq_len, hidden_size]\n    \n        # Pooling: Use the CLS token embedding or mean pooling\n        pooled_outputs = outputs[:, 0, :]  # Use CLS token embedding\n        # Alternatively, for mean pooling: pooled_outputs = outputs.mean(dim=1)\n    \n        # Reshape logits to [batch_size, num_choices]\n        logits = pooled_outputs.view(-1, num_choices, pooled_outputs.size(-1))  # [batch_size, num_choices, hidden_size]\n    \n        # Reduce logits to [batch_size, num_choices] (classification head)\n        logits = logits.mean(dim=-1)  # Reduce hidden_size dimension\n    \n        # Compute loss\n        loss = loss_fn(logits, labels)  # Labels shape: [batch_size]\n    \n        # Backward pass and optimizer step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # Calculate accuracy\n        predictions = torch.argmax(logits, dim=1)  # Shape: [batch_size]\n        correct_predictions = (predictions == labels).sum().item()\n        total_correct += correct_predictions\n        total_samples += labels.size(0)\n    \n    # Calculate epoch metrics\n    epoch_loss = total_loss / len(train_loader)\n    epoch_accuracy = total_correct / total_samples\n\n    # Print metrics for the epoch\n    print(f\"Epoch {epoch + 1}/{epochs}, Accuracy: {epoch_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:28.760784Z","iopub.execute_input":"2025-01-09T20:50:28.761028Z","iopub.status.idle":"2025-01-09T20:50:28.766437Z","shell.execute_reply.started":"2025-01-09T20:50:28.761007Z","shell.execute_reply":"2025-01-09T20:50:28.765601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. VIDEO PREPROCESSING & CLIP","metadata":{"_uuid":"b814f5db-94f8-45f9-a71b-bd4f80838676","_cell_guid":"882b7ab8-fd63-439d-90fc-1713d42f8fc3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Get the set of allowed GIF names from the DataFrame\ntrain_ra_allowed_titles = set(preprocessed_df_train_ra['title'].apply(lambda x: x[0] if isinstance(x, list) and x else None))\ntest_ra_allowed_titles = set(preprocessed_df_test_ra['title'].apply(lambda x: x[0] if isinstance(x, list) and x else None))","metadata":{"_uuid":"774bf605-0599-4985-b819-647976227ef8","_cell_guid":"6e589dd5-096b-4a87-8929-6e93bb27a64b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:28.767213Z","iopub.execute_input":"2025-01-09T20:50:28.767602Z","iopub.status.idle":"2025-01-09T20:50:28.787730Z","shell.execute_reply.started":"2025-01-09T20:50:28.767523Z","shell.execute_reply":"2025-01-09T20:50:28.786973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to TSV file\ntsv_file = \"/kaggle/input/tgif-mc-video/tgif-v1.0.tsv\"","metadata":{"_uuid":"771aaecb-416e-42f6-9196-3edec1758fb2","_cell_guid":"e4045665-5825-4256-893a-1a68b91bc5b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:28.803040Z","iopub.execute_input":"2025-01-09T20:50:28.803280Z","iopub.status.idle":"2025-01-09T20:50:28.817859Z","shell.execute_reply.started":"2025-01-09T20:50:28.803248Z","shell.execute_reply":"2025-01-09T20:50:28.816870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CLIP model and processor from Hugging Face\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")","metadata":{"_uuid":"17a81997-9740-43fe-b92e-481e2f16251c","_cell_guid":"8a874e2c-4c6b-43e6-9a46-965f1f23bed1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:28.818662Z","iopub.execute_input":"2025-01-09T20:50:28.818886Z","iopub.status.idle":"2025-01-09T20:50:34.392874Z","shell.execute_reply.started":"2025-01-09T20:50:28.818858Z","shell.execute_reply":"2025-01-09T20:50:34.392207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directories for downloaded train GIFs\noutput_dir_train_ra = \"/kaggle/working/downloaded_gifs_train_ra\"\nos.makedirs(output_dir_train_ra, exist_ok=True)","metadata":{"_uuid":"ef150d4c-862f-44eb-9eb8-0bda125da93f","_cell_guid":"293621cf-6972-4b3b-8954-476f75f1b6db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:34.393610Z","iopub.execute_input":"2025-01-09T20:50:34.393816Z","iopub.status.idle":"2025-01-09T20:50:34.398313Z","shell.execute_reply.started":"2025-01-09T20:50:34.393799Z","shell.execute_reply":"2025-01-09T20:50:34.397452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Download gifs","metadata":{}},{"cell_type":"code","source":"# Allowed titles per category\nallowed_titles_dict = {\n    \"train_ra\": train_ra_allowed_titles,\n}\n\n# Output directories for each category\ncategory_dirs = {\n    \"train_ra\": output_dir_train_ra,\n}","metadata":{"_uuid":"9edf7a2f-1ddb-44b0-934b-ad7c6ff2ac19","_cell_guid":"ad690541-fc6a-4dd0-a997-04bf570c468f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-09T20:50:34.399149Z","iopub.execute_input":"2025-01-09T20:50:34.399403Z","iopub.status.idle":"2025-01-09T20:50:34.417791Z","shell.execute_reply.started":"2025-01-09T20:50:34.399384Z","shell.execute_reply":"2025-01-09T20:50:34.416921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Get embeddings for frames extracted from gifs","metadata":{}},{"cell_type":"code","source":"def extract_video_embeddings(video_path, model, processor, device):\n    \"\"\"\n    Extract video embeddings from the provided video file.\n\n    Args:\n        video_path (str): Path to the video file.\n        model: Pre-trained model to generate embeddings.\n        processor: Pre-trained processor for the model.\n        device: CPU or GPU for computation.\n\n    Returns:\n        numpy.ndarray: Video embeddings for the video with shape (num_frames, embedding_size).\n    \"\"\"\n    video_obj = cv2.VideoCapture(video_path)\n    success, frame = True, None\n    frame_embeddings = []\n\n    while success:\n        success, frame = video_obj.read()\n        if not success or frame is None:\n            break\n\n        # Convert BGR to RGB\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Extract frame embedding\n        image_pil = Image.fromarray(frame)\n        inputs = processor(images=image_pil, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            outputs = model.get_image_features(**inputs)\n            embedding = outputs.cpu().numpy()\n        frame_embeddings.append(embedding)\n\n    video_obj.release()\n\n    if len(frame_embeddings) == 0:\n        # Return an empty array with the second dimension matching embedding size\n        return np.empty((0, model.get_image_features(**inputs).size(-1)))\n    else:\n        # Stack embeddings and ensure consistent shape\n        embeddings = np.vstack(frame_embeddings)\n        return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:34.418620Z","iopub.execute_input":"2025-01-09T20:50:34.418905Z","iopub.status.idle":"2025-01-09T20:50:34.432607Z","shell.execute_reply.started":"2025-01-09T20:50:34.418877Z","shell.execute_reply":"2025-01-09T20:50:34.431865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. DATASET OF TEXT-VIDEO EMBEDDINGS PAIRS","metadata":{"_uuid":"890fb43b-c1df-415c-b4ad-859f8146856c","_cell_guid":"14e64533-704b-4f39-8dcc-261987155026","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def Paired_Dataset_with_Embeddings(tsv_file, category_dirs, allowed_titles_dict, preprocessed_df_train_ra, train_dataset_ra, text_encoder, model, processor, device, max_length, gif_limit=20):\n    \"\"\"\n    Create a dataset of paired text embeddings, video embeddings, and labels with GIF alignment verification.\n    \n    Args:\n        tsv_file: Path to the TSV file containing GIF metadata.\n        category_dirs: Dictionary of category names and their directories.\n        allowed_titles_dict: Dictionary of allowed titles per category.\n        preprocessed_df_train_ra: DataFrame containing raw text and titles.\n        train_dataset_ra: Instance of MultipleChoiceDataset containing tokenized text data.\n        text_encoder: Instance of the TextEncoder model.\n        model: Pre-trained model for video embeddings.\n        processor: Pre-trained processor for video embeddings.\n        device: CPU or GPU for computation.\n        max_length: Maximum token length for text inputs.\n        gif_limit: Maximum number of GIFs to process.\n\n    Returns:\n        List of dictionaries with text embeddings, video embeddings, and labels.\n    \"\"\"\n    paired_dataset = []\n    counters = {category: 0 for category in category_dirs}\n    processed_gifs = 0\n\n    # Load the TSV file into a dictionary for fast lookup\n    gif_url_map = {}\n    with open(tsv_file, \"r\") as file:\n        for line in file:\n            gif_url = line.split(\"\\t\")[0]\n            gif_name_with_extension = os.path.basename(gif_url).replace(\"_\", \"\")\n            gif_name = gif_name_with_extension.rsplit('.', 1)[0]\n            gif_url_map[gif_name] = gif_url\n\n    # Move text_encoder to the correct device\n    text_encoder.to(device)\n\n    # Iterate over titles in preprocessed_df_train_ra\n    for idx, row in preprocessed_df_train_ra.iterrows():\n        title = row['title'][0]\n        if title not in gif_url_map:\n            continue\n\n        # Get the corresponding GIF URL and metadata\n        gif_url = gif_url_map[title]\n        gif_name_with_extension = os.path.basename(gif_url).replace(\"_\", \"\")\n        gif_name = gif_name_with_extension.rsplit('.', 1)[0]\n\n        # Identify the category for the GIF\n        category = None\n        for cat, allowed_titles in allowed_titles_dict.items():\n            if gif_name in allowed_titles:\n                category = cat\n                break\n        if category is None:\n            continue\n\n        # Download the GIF\n        gif_path = os.path.join(category_dirs[category], gif_name_with_extension)\n        try:\n            response = requests.get(gif_url, stream=True, timeout=10)\n            response.raise_for_status()\n            with open(gif_path, \"wb\") as gif_file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    gif_file.write(chunk)\n        except requests.RequestException as e:\n            print(f\"Failed to download {gif_url}: {e}\")\n            continue\n\n        # Extract video embeddings\n        try:\n            video_embeddings = extract_video_embeddings(gif_path, model, processor, device)\n        except Exception as e:\n            print(f\"Failed to extract embeddings for {gif_path}: {e}\")\n            continue\n\n        # Get tokenized data from train_dataset_ra\n        text_data = train_dataset_ra[idx]\n        input_ids = text_data[\"input_ids\"].to(device)\n        attention_mask = text_data[\"attention_mask\"].to(device)\n\n        # Compute text embeddings using the TextEncoder\n        with torch.no_grad():\n            text_embeddings = text_encoder(input_ids, attention_mask)  # Shape: [batch_size, seq_len, embed_dim]\n\n        # Add to paired dataset\n        paired_dataset.append({\n            \"text_embeddings\": text_embeddings.cpu(),  # Convert to CPU for storage\n            \"video_embeddings\": torch.tensor(video_embeddings, dtype=torch.float),\n            \"label\": text_data[\"labels\"]\n        })\n\n        counters[category] += 1\n        processed_gifs += 1\n        #print(f\"Processed title: {title}, Total Processed: {processed_gifs}/{gif_limit}\")\n\n        if processed_gifs >= gif_limit:\n            print(f\"Processed {gif_limit} GIFs. Stopping early.\")\n            break\n\n    print(f\"Paired dataset created. Counts per category: {counters}\")\n    return paired_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:34.433335Z","iopub.execute_input":"2025-01-09T20:50:34.433559Z","iopub.status.idle":"2025-01-09T20:50:34.446507Z","shell.execute_reply.started":"2025-01-09T20:50:34.433540Z","shell.execute_reply":"2025-01-09T20:50:34.445781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate paired dataset\npaired_dataset = Paired_Dataset_with_Embeddings(\n    tsv_file=tsv_file,\n    category_dirs=category_dirs,\n    allowed_titles_dict=allowed_titles_dict,\n    preprocessed_df_train_ra=preprocessed_df_train_ra,\n    train_dataset_ra=train_dataset_ra,\n    text_encoder=text_encoder,\n    model=model,\n    processor=processor,\n    device=device,\n    max_length=128,\n    gif_limit=1200\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T20:50:34.447226Z","iopub.execute_input":"2025-01-09T20:50:34.447461Z","iopub.status.idle":"2025-01-09T21:00:15.800917Z","shell.execute_reply.started":"2025-01-09T20:50:34.447434Z","shell.execute_reply":"2025-01-09T21:00:15.800142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Access the first element\nfirst_element = paired_dataset[0]\n\n# Print the entire element\nprint(\"First Element in Paired Dataset:\")\nprint(first_element)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.801774Z","iopub.execute_input":"2025-01-09T21:00:15.802002Z","iopub.status.idle":"2025-01-09T21:00:15.827352Z","shell.execute_reply.started":"2025-01-09T21:00:15.801982Z","shell.execute_reply":"2025-01-09T21:00:15.826649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print details of each component\nprint(\"\\nVideo Embeddings:\")\nprint(f\"Shape: {first_element['video_embeddings'].shape}\")\n\nprint(\"\\nText Embeddings:\")\nprint(f\"Shape: {first_element['text_embeddings'].shape}\")\n\nprint(\"\\nLabel:\")\nprint(first_element[\"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.828029Z","iopub.execute_input":"2025-01-09T21:00:15.828263Z","iopub.status.idle":"2025-01-09T21:00:15.834109Z","shell.execute_reply.started":"2025-01-09T21:00:15.828244Z","shell.execute_reply":"2025-01-09T21:00:15.833488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_shapes = []\n# Iterate through the paired_dataset and print the shape of video embeddings\nfor i, element in enumerate(paired_dataset):\n    # Access the column \n    video_shape = element[\"video_embeddings\"].shape  # Shape of video embeddings\n    video_shapes.append(video_shape)\n\nframe_counts = [shape[0] for shape in video_shapes]\nmean_frames = int(np.mean(frame_counts))\nprint(f\"Mean number of frames: {mean_frames}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.835029Z","iopub.execute_input":"2025-01-09T21:00:15.835261Z","iopub.status.idle":"2025-01-09T21:00:15.847725Z","shell.execute_reply.started":"2025-01-09T21:00:15.835243Z","shell.execute_reply":"2025-01-09T21:00:15.846912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract all labels from the paired_dataset\nlabels = [element[\"label\"].item() for element in paired_dataset]  # Assuming \"label\" is a tensor\n\n# Count occurrences of each label\nlabel_counts = Counter(labels)\n\n# Print the counts\nfor label, count in label_counts.items():\n    print(f\"Label {label}: {count} occurrences\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.848767Z","iopub.execute_input":"2025-01-09T21:00:15.849044Z","iopub.status.idle":"2025-01-09T21:00:15.863043Z","shell.execute_reply.started":"2025-01-09T21:00:15.849017Z","shell.execute_reply":"2025-01-09T21:00:15.862408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. CROSS-MODAL TRANSFORMER","metadata":{}},{"cell_type":"code","source":"class CrossModalConfig:\n    \"\"\"\n    Configuration for the Cross-Modal Transformer.\n    \"\"\"\n    def __init__(self, hidden_size, num_hidden_layers, num_attention_heads, dropout_prob, max_position_embeddings, num_labels, loss_type=\"ce\"):\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_dropout_prob = dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.num_labels = num_labels\n        self.loss_type = loss_type\n\n\nclass CrossModalEmbeddings(nn.Module):\n    def __init__(self, config):\n        super(CrossModalEmbeddings, self).__init__()\n        self.text_encoder = T5Model.from_pretrained(\"t5-small\").encoder  # Use only the encoder of T5\n        self.visual_embeddings = nn.Linear(512, 512)  # Project visual features\n        self.layer_norm = nn.LayerNorm(512)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, text_input_ids=None, text_attention_mask=None, visual_inputs=None, combined_embeddings=None):\n        if combined_embeddings is not None:\n            return self.layer_norm(self.dropout(combined_embeddings))\n    \n        # Use BERT for text embeddings\n        text_embeds = self.text_bert(input_ids=text_input_ids, attention_mask=text_attention_mask).last_hidden_state  # (batch_size, seq_len_text, hidden_size)\n    \n        # Ensure `visual_inputs` has the correct shape\n        if visual_inputs.dim() == 2:  # If visual inputs are 2D (batch_size, feature_dim)\n            visual_inputs = visual_inputs.unsqueeze(1)  # Add a sequence dimension: (batch_size, 1, feature_dim)\n\n        # Project visual features\n        visual_embeds = self.visual_embeddings(visual_inputs)  # Shape: (batch_size, seq_len_video, hidden_size)\n    \n        # Concatenate text and visual embeddings along the sequence dimension\n        combined_embeds = torch.cat((text_embeds, visual_embeds), dim=1)  # Shape: [batch_size, seq_len_text + seq_len_video, 768]\n    \n        return self.layer_norm(self.dropout(combined_embeds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.863873Z","iopub.execute_input":"2025-01-09T21:00:15.864115Z","iopub.status.idle":"2025-01-09T21:00:15.876781Z","shell.execute_reply.started":"2025-01-09T21:00:15.864097Z","shell.execute_reply":"2025-01-09T21:00:15.876188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossModalTransformerEncoderLayer(nn.Module):\n    \"\"\"\n    A single layer of the Cross-Modal Transformer encoder.\n    \"\"\"\n    def __init__(self, config):\n        super(CrossModalTransformerEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, dropout=config.hidden_dropout_prob, batch_first=True)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n            nn.ReLU(),\n            nn.Linear(4 * config.hidden_size, config.hidden_size)\n        )\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, attention_mask):\n        attn_output, _ = self.self_attn(hidden_states, hidden_states, hidden_states, key_padding_mask=attention_mask)\n        hidden_states = self.layer_norm_1(hidden_states + self.dropout(attn_output))\n        feed_forward_output = self.feed_forward(hidden_states)\n        hidden_states = self.layer_norm_2(hidden_states + self.dropout(feed_forward_output))\n        return hidden_states\n\n\nclass CrossModalTransformerEncoder(nn.Module):\n    \"\"\"\n    The encoder for the Cross-Modal Transformer.\n    \"\"\"\n    def __init__(self, config):\n        super(CrossModalTransformerEncoder, self).__init__()\n        self.layers = nn.ModuleList([CrossModalTransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask):\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.877640Z","iopub.execute_input":"2025-01-09T21:00:15.877915Z","iopub.status.idle":"2025-01-09T21:00:15.894397Z","shell.execute_reply.started":"2025-01-09T21:00:15.877889Z","shell.execute_reply":"2025-01-09T21:00:15.893630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossModalTransformerDecoderLayer(nn.Module):\n    \"\"\"\n    A single layer of the Cross-Modal Transformer decoder.\n    \"\"\"\n    def __init__(self, config):\n        super(CrossModalTransformerDecoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, dropout=config.hidden_dropout_prob, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, dropout=config.hidden_dropout_prob, batch_first=True)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n            nn.ReLU(),\n            nn.Linear(4 * config.hidden_size, config.hidden_size)\n        )\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, encoder_output, self_attention_mask, cross_attention_mask):\n        attn_output, _ = self.self_attn(hidden_states, hidden_states, hidden_states, key_padding_mask=self_attention_mask)\n        hidden_states = self.layer_norm_1(hidden_states + self.dropout(attn_output))\n\n        cross_attn_output, _ = self.cross_attn(hidden_states, encoder_output, encoder_output, key_padding_mask=cross_attention_mask)\n        hidden_states = self.layer_norm_2(hidden_states + self.dropout(cross_attn_output))\n\n        feed_forward_output = self.feed_forward(hidden_states)\n        hidden_states = self.layer_norm_3(hidden_states + self.dropout(feed_forward_output))\n\n        return hidden_states\n\n\nclass CrossModalTransformerDecoder(nn.Module):\n    \"\"\"\n    The decoder for the Cross-Modal Transformer.\n    \"\"\"\n    def __init__(self, config):\n        super(CrossModalTransformerDecoder, self).__init__()\n        self.layers = nn.ModuleList([CrossModalTransformerDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, encoder_output, self_attention_mask, cross_attention_mask):\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, encoder_output, self_attention_mask, cross_attention_mask)\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.895196Z","iopub.execute_input":"2025-01-09T21:00:15.895450Z","iopub.status.idle":"2025-01-09T21:00:15.911253Z","shell.execute_reply.started":"2025-01-09T21:00:15.895423Z","shell.execute_reply":"2025-01-09T21:00:15.910600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossModalTransformer(nn.Module):\n    def __init__(self, config):\n        super(CrossModalTransformer, self).__init__()\n        self.embeddings = CrossModalEmbeddings(config)\n        self.encoder = CrossModalTransformerEncoder(config)\n        self.decoder = CrossModalTransformerDecoder(config)\n        self.pooler = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size),\n            nn.Tanh(),\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size * 2),\n            nn.ReLU(),\n            nn.Linear(config.hidden_size * 2, config.num_labels),\n        )\n\n        # Add a projection layer for video embeddings\n        self.video_projection = nn.Linear(512, 512)\n        \n    def forward(self, text_embeddings=None, visual_embeddings=None, text_attention_mask=None, labels=None):\n        if text_embeddings is not None and visual_embeddings is not None:\n            # Project video embeddings to match text embeddings size\n            visual_embeddings = self.video_projection(visual_embeddings)\n    \n            # Use precomputed embeddings\n            combined_embeds = torch.cat((text_embeddings, visual_embeddings), dim=1)\n    \n            # Create attention masks\n            text_len = text_embeddings.size(1)\n            visual_len = visual_embeddings.size(1)\n    \n            # Generate visual attention mask if missing\n            if text_attention_mask is None:\n                text_attention_mask = torch.ones(\n                    (text_embeddings.size(0), text_len), device=text_embeddings.device\n                )\n            visual_attention_mask = torch.ones(\n                (visual_embeddings.size(0), visual_len), device=visual_embeddings.device\n            )\n    \n            # Concatenate text and visual attention masks\n            attention_mask = torch.cat((text_attention_mask, visual_attention_mask), dim=1)  # [batch_size, text_len + visual_len]\n        else:\n            raise ValueError(\"Both text_embeddings and visual_embeddings must be provided.\")\n    \n        # Encoder\n        encoder_output = self.encoder(\n            combined_embeds, attention_mask == 0  # Multi-head attention expects True for padding\n        )\n\n        # Decoder\n        decoder_output = self.decoder(combined_embeds, encoder_output, attention_mask == 0, attention_mask == 0)\n    \n        # Pooling (use CLS token or mean pooling)\n        pooled_output = self.pooler(decoder_output[:, 0, :])  # Use CLS token embedding\n    \n        # Classification head\n        logits = self.classifier(pooled_output)\n    \n        # Loss calculation\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n    \n        return {\"logits\": logits, \"loss\": loss}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.911978Z","iopub.execute_input":"2025-01-09T21:00:15.912348Z","iopub.status.idle":"2025-01-09T21:00:15.931415Z","shell.execute_reply.started":"2025-01-09T21:00:15.912318Z","shell.execute_reply":"2025-01-09T21:00:15.930639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch, num_choices=5, max_frames=20):\n    valid_batch = [item for item in batch if item[\"label\"] >= 0]  # Filter out invalid labels\n    if not valid_batch:\n        raise ValueError(\"No valid samples in the batch.\")\n\n    # Extract and stack text embeddings\n    text_embeddings = torch.stack([item[\"text_embeddings\"] for item in valid_batch])  # Shape: [batch_size, num_choices, seq_len, hidden_size]\n\n    # Repeat video embeddings to align with num_choices\n    video_embeddings = []\n    for item in valid_batch:\n        embeddings = item[\"video_embeddings\"]\n        if embeddings.shape[0] > max_frames:\n            embeddings = embeddings[:max_frames]  # Truncate\n        else:\n            pad_length = max_frames - embeddings.shape[0]\n            padding = torch.zeros((pad_length, embeddings.shape[1]))  # Pad with zeros\n            embeddings = torch.cat((embeddings, padding), dim=0)\n\n        # Repeat the video embeddings for each choice\n        video_embeddings.extend([embeddings] * num_choices)\n    video_embeddings = torch.stack(video_embeddings)  # Shape: [batch_size * num_choices, max_frames, hidden_size]\n\n    # Flatten text embeddings\n    batch_size, num_choices, seq_len, hidden_size = text_embeddings.shape\n    text_embeddings = text_embeddings.view(batch_size * num_choices, seq_len, hidden_size)\n\n    # Repeat labels for num_choices\n    labels = torch.tensor([item[\"label\"] for item in valid_batch], dtype=torch.long)\n    labels = labels.repeat_interleave(num_choices)  # Repeat labels for each choice\n\n    return text_embeddings, video_embeddings, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.932175Z","iopub.execute_input":"2025-01-09T21:00:15.932474Z","iopub.status.idle":"2025-01-09T21:00:15.952014Z","shell.execute_reply.started":"2025-01-09T21:00:15.932444Z","shell.execute_reply":"2025-01-09T21:00:15.951403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataLoader\ntrain_loader = DataLoader(paired_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.952675Z","iopub.execute_input":"2025-01-09T21:00:15.952938Z","iopub.status.idle":"2025-01-09T21:00:15.972662Z","shell.execute_reply.started":"2025-01-09T21:00:15.952918Z","shell.execute_reply":"2025-01-09T21:00:15.971863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\nconfig = CrossModalConfig(\n    hidden_size=768,\n    num_hidden_layers=6,\n    num_attention_heads=8,\n    dropout_prob=0.1,\n    max_position_embeddings=512,\n    num_labels=5,  # Number of classification labels\n)\ncmt_model = CrossModalTransformer(config).to(device)\n\n# Optimizer\noptimizer = torch.optim.AdamW(cmt_model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:15.973445Z","iopub.execute_input":"2025-01-09T21:00:15.973685Z","iopub.status.idle":"2025-01-09T21:00:17.403445Z","shell.execute_reply.started":"2025-01-09T21:00:15.973655Z","shell.execute_reply":"2025-01-09T21:00:17.402520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lists to store metrics\nlosses = []\naccuracies = []\n\n# Training loop\nepochs = 100\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\nscaler = GradScaler(\"cuda\")  # Explicitly specify the device\n\nfor epoch in range(epochs):\n    cmt_model.train()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    for text_embeddings, video_embeddings, labels in train_loader:\n        optimizer.zero_grad()  # Zero the gradients before each batch\n        text_embeddings = text_embeddings.to(device)\n        video_embeddings = video_embeddings.to(device)\n        labels = labels.to(device)\n\n        with autocast(device_type=\"cuda\"):  # Correct usage of autocast\n            # Forward pass\n            outputs = cmt_model(\n                text_embeddings=text_embeddings,\n                visual_embeddings=video_embeddings,\n                text_attention_mask=None,  # Precomputed embeddings don't need attention masks\n                labels=labels,\n            )\n\n            # Loss\n            loss = outputs[\"loss\"]\n            total_loss += loss.item()\n\n        # Scaler: Scale the loss and backpropagate\n        scaler.scale(loss).backward()\n\n        # Optimizer step with scaler\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Accuracy\n        _, predicted = torch.max(outputs[\"logits\"], dim=1)  # Get predictions\n        total_correct += (predicted == labels).sum().item()  # Count correct predictions\n        total_samples += labels.size(0)  # Total samples in batch\n\n    scheduler.step()\n\n    # Compute average loss and accuracy for the epoch\n    avg_loss = total_loss / len(train_loader)\n    accuracy = total_correct / total_samples\n\n    # Store metrics\n    losses.append(avg_loss)\n    accuracies.append(accuracy)\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:00:17.404385Z","iopub.execute_input":"2025-01-09T21:00:17.404701Z","iopub.status.idle":"2025-01-09T21:26:35.971269Z","shell.execute_reply.started":"2025-01-09T21:00:17.404669Z","shell.execute_reply":"2025-01-09T21:26:35.970459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a directory to save the model\nsave_dir = \"/kaggle/working/\"\nos.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\n# Save the model\ntorch.save(cmt_model.state_dict(), os.path.join(save_dir, \"cross_modal_transformer.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:35.972136Z","iopub.execute_input":"2025-01-09T21:26:35.972381Z","iopub.status.idle":"2025-01-09T21:26:37.056686Z","shell.execute_reply.started":"2025-01-09T21:26:35.972359Z","shell.execute_reply":"2025-01-09T21:26:37.055842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot and save Loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, epochs + 1), losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.legend()\nplt.savefig(\"/kaggle/working/training_loss.png\")  # Save the loss plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.057678Z","iopub.execute_input":"2025-01-09T21:26:37.057979Z","iopub.status.idle":"2025-01-09T21:26:37.419788Z","shell.execute_reply.started":"2025-01-09T21:26:37.057948Z","shell.execute_reply":"2025-01-09T21:26:37.418966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot and save Accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, epochs + 1), accuracies, label=\"Training Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy Over Epochs\")\nplt.legend()\nplt.savefig(\"/kaggle/working/training_accuracy.png\")  # Save the accuracy plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.420538Z","iopub.execute_input":"2025-01-09T21:26:37.420817Z","iopub.status.idle":"2025-01-09T21:26:37.663746Z","shell.execute_reply.started":"2025-01-09T21:26:37.420784Z","shell.execute_reply":"2025-01-09T21:26:37.662887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8. EVALUATING THE MODEL","metadata":{}},{"cell_type":"code","source":"# prepare data\ntest_dataset_ra = MultipleChoiceDataset(preprocessed_df_test_ra, tokenizer)\ntest_ra_allowed_titles = set(preprocessed_df_test_ra['title'].apply(lambda x: x[0] if isinstance(x, list) and x else None))\noutput_dir_test_ra = \"/kaggle/working/downloaded_gifs_test_ra\"\nos.makedirs(output_dir_test_ra, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.664454Z","iopub.execute_input":"2025-01-09T21:26:37.664672Z","iopub.status.idle":"2025-01-09T21:27:45.211368Z","shell.execute_reply.started":"2025-01-09T21:26:37.664653Z","shell.execute_reply":"2025-01-09T21:27:45.210578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Allowed titles per category\nallowed_titles_dict = {\n    \"test_ra\": test_ra_allowed_titles\n}\n\n# Output directories for each category\ncategory_dirs = {\n    \"test_ra\": output_dir_test_ra\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.664454Z","iopub.execute_input":"2025-01-09T21:26:37.664672Z","iopub.status.idle":"2025-01-09T21:27:45.211368Z","shell.execute_reply.started":"2025-01-09T21:26:37.664653Z","shell.execute_reply":"2025-01-09T21:27:45.210578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate paired dataset\npaired_dataset_test = Paired_Dataset_with_Embeddings(\n    tsv_file=tsv_file,\n    category_dirs=category_dirs,\n    allowed_titles_dict=allowed_titles_dict,\n    preprocessed_df_train_ra=preprocessed_df_test_ra,\n    train_dataset_ra=test_dataset_ra,\n    text_encoder=text_encoder,\n    model=model,\n    processor=processor,\n    device=device,\n    max_length=128,\n    gif_limit=300\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.664454Z","iopub.execute_input":"2025-01-09T21:26:37.664672Z","iopub.status.idle":"2025-01-09T21:27:45.211368Z","shell.execute_reply.started":"2025-01-09T21:26:37.664653Z","shell.execute_reply":"2025-01-09T21:27:45.210578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a DataLoader for test data\ntest_loader = DataLoader(\n    paired_dataset_test, batch_size=16, shuffle=False, collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:26:37.664454Z","iopub.execute_input":"2025-01-09T21:26:37.664672Z","iopub.status.idle":"2025-01-09T21:27:45.211368Z","shell.execute_reply.started":"2025-01-09T21:26:37.664653Z","shell.execute_reply":"2025-01-09T21:27:45.210578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# directory in wich the trained model is stored\nmodel_dir = '/kaggle/working/cross_modal_transformer.pth'\n\n# Define the model configuration (must match the saved model's configuration)\ntest_config = CrossModalConfig(\n    hidden_size=768,\n    num_hidden_layers=6,\n    num_attention_heads=8,\n    dropout_prob=0.1,\n    max_position_embeddings=512,\n    num_labels=5,  # Number of classification labels\n)\n\n# Initialize the model\ncmt_model_test = CrossModalTransformer(test_config).to(device)\n\n# Load the saved model weights\ncmt_model_test.load_state_dict(torch.load(model_dir))\ncmt_model_test.eval()  # Set the model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:27:45.212045Z","iopub.execute_input":"2025-01-09T21:27:45.212296Z","iopub.status.idle":"2025-01-09T21:27:47.266037Z","shell.execute_reply.started":"2025-01-09T21:27:45.212275Z","shell.execute_reply":"2025-01-09T21:27:47.265187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_correct = 0\ntotal_samples = 0\nlosses = []\nloss_fn = nn.CrossEntropyLoss()\n\n# Put model in evaluation mode\ncmt_model_test.eval()\n\nwith torch.no_grad():\n    for text_embeddings, video_embeddings, labels in test_loader:\n        text_embeddings = text_embeddings.to(device)\n        video_embeddings = video_embeddings.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        with autocast(\"cuda\"):\n            outputs = cmt_model_test(\n                text_embeddings=text_embeddings,\n                visual_embeddings=video_embeddings,\n                text_attention_mask=None,  # Precomputed embeddings don't need attention masks\n                labels=labels,\n            )\n\n        # Loss\n        loss = outputs[\"loss\"]\n        losses.append(loss.item())\n\n        # Accuracy\n        _, predicted = torch.max(outputs[\"logits\"], dim=1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n# Calculate test accuracy and loss\ntest_loss = sum(losses) / len(losses)\ntest_accuracy = total_correct / total_samples\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:27:47.266949Z","iopub.execute_input":"2025-01-09T21:27:47.267293Z","iopub.status.idle":"2025-01-09T21:27:48.850503Z","shell.execute_reply.started":"2025-01-09T21:27:47.267255Z","shell.execute_reply":"2025-01-09T21:27:48.849608Z"}},"outputs":[],"execution_count":null}]}